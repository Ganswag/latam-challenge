{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este archivo puedes escribir lo que estimes conveniente. Te recomendamos detallar tu solución y todas las suposiciones que estás considerando. Aquí puedes ejecutar las funciones que definiste en los otros archivos de la carpeta src, medir el tiempo, memoria, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's start with a handmade aproach\n",
    "\n",
    "Let's suppose we are in a truly limited computer, but with infinite execution time\n",
    "\n",
    "But first, let's check a suposition about dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "FILE_PATH = './farmers-protest-tweets-2021-2-4.json'\n",
    "FILE_PATH_TEST = './farmers-protest-tweets-2021-2-4-test.json' # An smaller dataset :P\n",
    "READ_MODE = 'r'\n",
    "DUMMY_DATE = datetime.fromisoformat('2050-01-01T00:00:00+00:00').date()\n",
    "TOP_N = 10\n",
    "\n",
    "# Let's define a fictional date in the future to compare\n",
    "prev_date = DUMMY_DATE\n",
    "\n",
    "with open(FILE_PATH, 'r', encoding='utf-8', buffering=1) as json_file:\n",
    "    for line in json_file:\n",
    "        _line = json.loads(line)\n",
    "\n",
    "        date = datetime.fromisoformat(_line['date']).date()\n",
    "\n",
    "        if prev_date < date:  # Check if is sorted in a desc way\n",
    "            raise ValueError\n",
    "        \n",
    "        prev_date = date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this was executed without errors, we can affirm dates are always desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to create a method to insert in a sorted way elements, to avoid to order\n",
    "\n",
    "... and create some basic tests for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_sorted(iterable: list, element: tuple, el_idx: int) -> list:\n",
    "    i = 0\n",
    "    element[el_idx]  # check if element index exists and raise error if not\n",
    "    \n",
    "    if not iterable or element[el_idx] < iterable[-1][el_idx]:\n",
    "        iterable.append(element)\n",
    "    else:\n",
    "        while element[el_idx] < iterable[i][el_idx] and i < len(iterable):\n",
    "            i += 1\n",
    "        iterable.insert(i, element)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "from unittest import TestCase\n",
    "\n",
    "class TestInsertSorted(TestCase):\n",
    "\n",
    "    def test_insert_sorted_initial(self):\n",
    "        INITIAL_TEST_VALUE = []\n",
    "        NEW_DATA = ('a', 1)\n",
    "        EXPECTED_RESULT = [('a', 1)]\n",
    "\n",
    "        test_value = INITIAL_TEST_VALUE.copy()\n",
    "        insert_sorted(test_value, NEW_DATA, 0)\n",
    "        self.assertEqual(test_value, EXPECTED_RESULT)\n",
    "\n",
    "        test_value = INITIAL_TEST_VALUE.copy()\n",
    "        insert_sorted(test_value, NEW_DATA, 1)\n",
    "        self.assertEqual(test_value, EXPECTED_RESULT)\n",
    "\n",
    "        test_value = INITIAL_TEST_VALUE.copy()\n",
    "        self.assertRaises(IndexError, insert_sorted, test_value, NEW_DATA, 2)\n",
    "\n",
    "    def test_insert_sorted_begin(self):\n",
    "        INITIAL_TEST_VALUE = [('c', 3),  ('b', 2), ('a', 1)]\n",
    "        NEW_DATA = ('d', 4)\n",
    "        EXPECTED_RESULT = [('d', 4),('c', 3),  ('b', 2), ('a', 1)]\n",
    "\n",
    "        test_value = INITIAL_TEST_VALUE.copy()\n",
    "        insert_sorted(test_value, NEW_DATA, 0)\n",
    "        self.assertEqual(test_value, EXPECTED_RESULT)\n",
    "\n",
    "        test_value = INITIAL_TEST_VALUE.copy()\n",
    "        insert_sorted(test_value, NEW_DATA, 1)\n",
    "        self.assertEqual(test_value, EXPECTED_RESULT)\n",
    "\n",
    "        test_value = INITIAL_TEST_VALUE.copy()\n",
    "        self.assertRaises(IndexError, insert_sorted, test_value, NEW_DATA, 2)\n",
    "\n",
    "\n",
    "    def test_insert_sorted_normal(self):\n",
    "        INITIAL_TEST_VALUE = [('d', 4), ('b', 2), ('a', 1)]\n",
    "        NEW_DATA = ('c', 3)\n",
    "        EXPECTED_RESULT = [('d', 4), ('c', 3), ('b', 2), ('a', 1)]\n",
    "\n",
    "        test_value = INITIAL_TEST_VALUE.copy()\n",
    "        insert_sorted(test_value, NEW_DATA, 0)\n",
    "        self.assertEqual(test_value, EXPECTED_RESULT)\n",
    "\n",
    "        test_value = INITIAL_TEST_VALUE.copy()\n",
    "        insert_sorted(test_value, NEW_DATA, 1)\n",
    "        self.assertEqual(test_value, EXPECTED_RESULT)\n",
    "\n",
    "        test_value = INITIAL_TEST_VALUE.copy()\n",
    "        self.assertRaises(IndexError, insert_sorted, test_value, NEW_DATA, 2)\n",
    "\n",
    "\n",
    "    def test_insert_sorted_end(self):\n",
    "        INITIAL_TEST_VALUE = [('d', 4), ('c', 3), ('b', 2)]\n",
    "        NEW_DATA = ('a', 1)\n",
    "        EXPECTED_RESULT = [('d', 4),('c', 3),  ('b', 2), ('a', 1)]\n",
    "\n",
    "        test_value = INITIAL_TEST_VALUE.copy()\n",
    "        insert_sorted(test_value, NEW_DATA, 0)\n",
    "        self.assertEqual(test_value, EXPECTED_RESULT)\n",
    "\n",
    "        test_value = INITIAL_TEST_VALUE.copy()\n",
    "        insert_sorted(test_value, NEW_DATA, 1)\n",
    "        self.assertEqual(test_value, EXPECTED_RESULT)\n",
    "\n",
    "        test_value = INITIAL_TEST_VALUE.copy()\n",
    "        self.assertRaises(IndexError, insert_sorted, test_value, NEW_DATA, 2)\n",
    "\n",
    "\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, everything looks fine...\n",
    "And we'll need to get the max value from tuples inside the value in a dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_value(tweets_counter_by_user: dict) -> tuple:\n",
    "    \"\"\" Returns a tuple (key, value) for the maximum value in a dictionay\n",
    "\n",
    "    WARNING: Keep in mind this only will work for positive numbers because\n",
    "    tweet counter cannot be negative.\n",
    "    \"\"\"\n",
    "    if not tweets_counter_by_user:\n",
    "        return None\n",
    "\n",
    "    max_value = ('__initial__', -1)\n",
    "    for user_name, tweets_counter in tweets_counter_by_user.items():\n",
    "        if tweets_counter > max_value[1]:  # This implies we'll get the first founded max\n",
    "            max_value = (user_name, tweets_counter)\n",
    "\n",
    "    return max_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "from unittest import TestCase\n",
    "\n",
    "class TestGetMaxValue(TestCase):\n",
    "\n",
    "    def test_get_max(self):\n",
    "        TEST_OBJECT = {\n",
    "            'min': 1,\n",
    "            'dummy': 2,\n",
    "            'max': 3\n",
    "        }\n",
    "        EXPECTED_RESULT = ('max', 3)\n",
    "        result = get_max_value(TEST_OBJECT)\n",
    "\n",
    "        self.assertEqual(result, EXPECTED_RESULT)\n",
    "\n",
    "    def test_get_duplicated(self):\n",
    "        TEST_OBJECT = {\n",
    "            'min': 1,\n",
    "            'dummy': 2,\n",
    "            'max': 2\n",
    "        }\n",
    "        EXPECTED_RESULT = ('dummy', 2)\n",
    "        result = get_max_value(TEST_OBJECT)\n",
    "\n",
    "        self.assertEqual(result, EXPECTED_RESULT)\n",
    "\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limited memory, a lot of execution time\n",
    "\n",
    "For this section let's suppose we have only a small computer but with infinite execution time\n",
    "\n",
    "Let's start with the q1 tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNTER_INDEX = 1\n",
    "DATE_INDEX = 0\n",
    "TOP_N = 10\n",
    "TOP_USERNAME_INDEX = 2\n",
    "USERNAME_INDEX = 0\n",
    "\n",
    "tweets_counter_by_user = {}\n",
    "max_user_by_date = []\n",
    "tweets_by_day = 0\n",
    "prev_date = DUMMY_DATE\n",
    "\n",
    "\n",
    "with open(FILE_PATH, 'r', encoding='utf-8', buffering=1) as json_file:\n",
    "    for line in json_file:\n",
    "        tweets_by_day += 1\n",
    "        _line = json.loads(line)\n",
    "\n",
    "        date = datetime.fromisoformat(_line['date']).date()\n",
    "        user = _line['user']['username']\n",
    "\n",
    "        # this is because date is always descendent:\n",
    "        if prev_date != date and prev_date != DUMMY_DATE:\n",
    "            max_value = get_max_value(tweets_counter_by_user)\n",
    "            insert_sorted(\n",
    "                max_user_by_date,\n",
    "                (prev_date, tweets_by_day - 1, max_value[USERNAME_INDEX]),\n",
    "                COUNTER_INDEX\n",
    "            )\n",
    "            if len(max_user_by_date) > TOP_N:  # We only want TOP_N results\n",
    "                max_user_by_date.pop()\n",
    "\n",
    "            tweets_counter_by_user = {}\n",
    "            tweets_by_day = 1\n",
    "\n",
    "        prev_date = date\n",
    "\n",
    "        if tweets_counter_by_user.get(user):\n",
    "            tweets_counter_by_user[user] += 1\n",
    "        else:\n",
    "            tweets_counter_by_user[user] = 1\n",
    "\n",
    "    # Compute data for last date:\n",
    "    max_value = get_max_value(tweets_counter_by_user)\n",
    "    insert_sorted(\n",
    "        max_user_by_date,\n",
    "        (prev_date, tweets_by_day - 1, max_value[USERNAME_INDEX]),\n",
    "        COUNTER_INDEX\n",
    "    )\n",
    "    if len(max_user_by_date) > TOP_N:  # We only want TOP_N results\n",
    "        max_user_by_date.pop()\n",
    "\n",
    "\n",
    "result = [(el[DATE_INDEX], el[TOP_USERNAME_INDEX]) for el in max_user_by_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now q3 because I don't know (yet) how to handle with emojies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "COUNTER_INDEX = 1\n",
    "mentions_counter = {}\n",
    "\n",
    "with open(FILE_PATH, 'r', encoding='utf-8', buffering=1) as json_file:\n",
    "    for line in json_file:\n",
    "        _line = json.loads(line)\n",
    "        tweet_content = _line['renderedContent']\n",
    "        mentioned_users = re.findall(r'@(\\w+)\\b', tweet_content)\n",
    "\n",
    "        for user in mentioned_users:\n",
    "            if mentions_counter.get(user):\n",
    "                mentions_counter[user] += 1\n",
    "            else:\n",
    "                mentions_counter[user] = 1\n",
    "\n",
    "all_mentions_sorted = {\n",
    "    username: mentions_counter for username, mentions_counter in \n",
    "    sorted(mentions_counter.items(), key=lambda item: item[COUNTER_INDEX], reverse=True)\n",
    "}\n",
    "top_n_mentions = [\n",
    "    (username, all_mentions_sorted[username]) \n",
    "    for username in list(all_mentions_sorted)[:TOP_N]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_mentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's copy the last solution but searching for emojis instead mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "\n",
    "\n",
    "emoji_counter = {}\n",
    "\n",
    "with open(FILE_PATH, 'r', encoding='utf-8', buffering=1) as json_file:\n",
    "    for line in json_file:\n",
    "        tweets_by_day += 1\n",
    "        _line = json.loads(line)\n",
    "        tweet_content = _line['renderedContent']\n",
    "\n",
    "        # This is trully slow, but since Indi characters are used I couldn't \n",
    "        # find a better solution far now :(\n",
    "        # I'd like to use a regex to find just emojies\n",
    "        used_emojis = [el['emoji'] for el in emoji.emoji_list(tweet_content)]\n",
    "\n",
    "        for _emoji in used_emojis:\n",
    "            if emoji_counter.get(_emoji):\n",
    "                emoji_counter[_emoji] += 1\n",
    "            else:\n",
    "                emoji_counter[_emoji] = 1\n",
    "\n",
    "# ToDo: Check if is possible to avoid sort at the end and keep top_n sorted\n",
    "sorted_emojies = {\n",
    "    emoji: counter for emoji, counter in sorted(\n",
    "        emoji_counter.items(), key=lambda item: item[COUNTER_INDEX],\n",
    "        reverse=True\n",
    "    )\n",
    "}\n",
    "\n",
    "# Format in the desired output\n",
    "top_n_emojies = [\n",
    "    (emoji, sorted_emojies[emoji]) for emoji in list(sorted_emojies)[:TOP_N]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_emojies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's suppose we have GCP and unlimited memory and procesors.\n",
    "\n",
    "Please note I don't believe to use GCP in this test could improve something since we are using an small dataset and limited computational resources, but this will be coded as if I wanted to use DataFlow and use as many workers as I wish, but instead read file from an Bucket, this time will be readed from a local path and the outoput just printed, but this could be stored in some GCP service easily, like BigQuery.\n",
    "\n",
    "Also remember the execution time will depends on the available computational resources, I believe the optimal solution for time and memory with limited resources are the previus ones, but lets use power that [apache beam](https://beam.apache.org/about/) offers.\n",
    "\n",
    "For more information about the execution model, take a look to [apache beam domentation](https://beam.apache.org/documentation/runtime/model/) about this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start with the easy one.\n",
    "\n",
    "Since multiple agregations are not required and we have a library to get emojis...\n",
    "Let's use a \"count words\" but with some extra steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apache_beam import CombinePerKey, Pipeline, FlatMap, Map\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "import apache_beam as beam\n",
    "\n",
    "import emoji\n",
    "\n",
    "def parse_element(element):\n",
    "    element = json.loads(element)\n",
    "    return element['renderedContent']\n",
    "\n",
    "\n",
    "def run(pipeline_args=None):\n",
    "    pipeline_options = PipelineOptions(\n",
    "        pipeline_args, save_main_session=True\n",
    "    )\n",
    "\n",
    "    with Pipeline(options=pipeline_options) as pipeline:\n",
    "        (\n",
    "            pipeline\n",
    "            | 'Read file' >> ReadFromText(FILE_PATH)  # This is faster than ReadFromJson\n",
    "            | 'Use the desired field' >> Map(parse_element)\n",
    "            # # Next line returns arrays:\n",
    "            | 'Get emojis' >> Map(lambda x: [el['emoji'] for el in emoji.emoji_list(x)])  # This is taking so much time :(\n",
    "            # # But next line flattens it:\n",
    "            | 'Flat elements' >> FlatMap(lambda x: x)\n",
    "            | 'Map to key / value' >> Map(lambda x: (x, 1))\n",
    "            | 'And sum by key' >> CombinePerKey(sum)\n",
    "            | \"Select top N\" >> beam.combiners.Top.Largest(TOP_N, key=lambda x: x[1])\n",
    "            | Map(print)\n",
    "        )\n",
    "\n",
    "\n",
    "pipeline_args = {'flexrs_goal': 'SPEED_OPTIMIZED'}  # Define this to avoid error\n",
    "run(pipeline_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "from apache_beam import CombinePerKey, Pipeline, FlatMap, Map, Regex\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "\n",
    "def run(pipeline_args=None):\n",
    "    pipeline_options = PipelineOptions(\n",
    "        pipeline_args, save_main_session=True,\n",
    "    )\n",
    "\n",
    "    with Pipeline(options=pipeline_options) as pipeline:\n",
    "        (\n",
    "            pipeline\n",
    "            | 'Read file' >> ReadFromText(FILE_PATH)\n",
    "            | 'Use the desired field' >> Map(parse_element)\n",
    "            # Next line returns arrays:\n",
    "            | 'Get mentions' >> Regex.find_all(r'(?<=@)(\\w+)\\b')\n",
    "            # But next line flattens it:\n",
    "            | 'Flat elements' >> FlatMap(lambda x: x)\n",
    "            | 'Map to (key, 1)' >> Map(lambda x: (x, 1))\n",
    "            | 'And sum by key' >> CombinePerKey(sum)\n",
    "            | \"Select top N\" >> beam.combiners.Top.Largest(TOP_N, key=lambda x: x[1])\n",
    "            | Map(print)\n",
    "            # ToDo: you could write it easily into BQ instead print it\n",
    "            # | WriteToBigQuery(\n",
    "            # project=project_id,\n",
    "            # table=table,\n",
    "            # schema=TABLE_SCHEMA,\n",
    "            # method=WriteToBigQuery.Method.STREAMING_INSERTS,  # To avoid use cloud storage \n",
    "            # insert_retry_strategy=RetryStrategy.RETRY_NEVER\n",
    "            # )\n",
    "        )\n",
    "\n",
    "pipeline_args = {'flexrs_goal': 'SPEED_OPTIMIZED'}  # Define this to avoid error\n",
    "run(pipeline_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But for get some top days with most activity and top user by day some extra ticks are needed, because we need to create a custom CombineFM, we take the solution from [Apache Beam documentation](https://beam.apache.org/documentation/transforms/python/aggregation/combinevalues/) looking in the [git repository](https://github.com/apache/beam/blob/master/sdks/python/apache_beam/examples/snippets/transforms/aggregation/combinevalues_combinefn.py) and adapt it to get what we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import apache_beam as beam\n",
    "from apache_beam import Pipeline, Map, GroupBy, CombineValues, CombineFn\n",
    "from apache_beam.io import ReadFromText, WriteToText\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "\n",
    "def parse_element(element):\n",
    "    element = json.loads(element)\n",
    "    return (\n",
    "        str(datetime.fromisoformat(element['date']).date()),\n",
    "        element['user']['username']\n",
    "    )\n",
    "\n",
    "\n",
    "def format_output(element):\n",
    "    DATE_INDEX = 0\n",
    "    MOST_ACTIVE_USER_INDEX, USERNAME_INDEX = 1\n",
    "\n",
    "    return [\n",
    "        (\n",
    "           datetime.strptime(el[DATE_INDEX], '%Y-%m-%d').date(),\n",
    "           el[MOST_ACTIVE_USER_INDEX][USERNAME_INDEX]\n",
    "        ) for el in element\n",
    "    ]\n",
    "\n",
    "\n",
    "class MaxFn(CombineFn):\n",
    "    def create_accumulator(self):\n",
    "      return {}\n",
    "\n",
    "    def add_input(self, accumulator, input):\n",
    "        if input not in accumulator:\n",
    "            accumulator[input] = 0\n",
    "        accumulator[input] += 1\n",
    "        return accumulator\n",
    "\n",
    "    def merge_accumulators(self, accumulators):\n",
    "      merged = {}\n",
    "      for accum in accumulators:\n",
    "        for item, count in accum.items():\n",
    "            if item not in merged:\n",
    "                merged[item] = 0\n",
    "            merged[item] += count\n",
    "      return merged\n",
    "\n",
    "    def extract_output(self, accumulator):\n",
    "        \"\"\"Customed solution to get the most active user by date\"\"\"\n",
    "        sum_tweets = 0\n",
    "\n",
    "        max_value = -1  # Initialize a dummy max value\n",
    "        most_active_user = None\n",
    "        for element, counter in accumulator.items():\n",
    "           sum_tweets += counter\n",
    "           if counter > max_value:\n",
    "                most_active_user = element[1]\n",
    "                max_value = counter\n",
    "\n",
    "        return sum_tweets, most_active_user\n",
    "\n",
    "\n",
    "def run(pipeline_args=None):\n",
    "    pipeline_options = PipelineOptions(pipeline_args)\n",
    "\n",
    "    with Pipeline(options=pipeline_options) as pipeline:\n",
    "        (\n",
    "            pipeline\n",
    "            # ToDo: You can replace read from local storage and read it from Cloud Storage using a gs://{path}\n",
    "            | 'Read file' >> ReadFromText(FILE_PATH)\n",
    "            | 'Use the desired field' >> Map(parse_element)\n",
    "            | 'Group all users by date' >> GroupBy(lambda x: x[0])\n",
    "            | 'Get most active user per date' >> CombineValues(MaxFn())\n",
    "            | 'Select Top 10' >> beam.combiners.Top.Largest(TOP_N, key=lambda x: x[1][0])\n",
    "            | Map(print)\n",
    "            # | 'Format to expected output' >> Map(format_output)\n",
    "            # # Since return a value is a nonsense in Apache Beam, let's persist the results:\n",
    "            # | 'Persist results' >> WriteToText('./results/q1/results.txt')\n",
    "\n",
    "            # ToDo: you could write it easily into BQ instead a file with this:\n",
    "            # 'Write to BQ' | WriteToBigQuery(\n",
    "            # project=project_id,\n",
    "            # table=table,\n",
    "            # schema=TABLE_SCHEMA,\n",
    "            # method=WriteToBigQuery.Method.STREAMING_INSERTS,  # To avoid use cloud storage \n",
    "            # insert_retry_strategy=RetryStrategy.RETRY_NEVER\n",
    "            # )\n",
    "        )\n",
    "\n",
    "\n",
    "pipeline_args = {'flexrs_goal': 'SPEED_OPTIMIZED'}  # Define this to avoid error\n",
    "run(pipeline_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check we can read the file\n",
    "\n",
    "import os\n",
    "\n",
    "files_paths = [x for x in os.listdir('./results/q1/')]\n",
    "file_path = os.path.join('./results/q1/' , files_paths[0])\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8', buffering=1) as file:\n",
    "    import datetime\n",
    "\n",
    "    for line in file:\n",
    "        result = eval(line)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important consideration:\n",
    "\n",
    "Since Apache dataflow doesn't returns an element, we'll write the results into a file and then read it :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCP deployment instructions\n",
    "\n",
    "The apache beam solutions could be deployed using Flex Templates\n",
    "\n",
    " ```bash\n",
    "export PROJECT=my-project-id\n",
    "export BUCKET_NAME=dataflow-utilities\n",
    "export BUCKET_PATH=dataflow/templates\n",
    "export TEMPLATE_NAME=latam-challenge\n",
    "export REGION=us-east1\n",
    "```\n",
    "\n",
    "Create a new bucket to save the template\n",
    "```bash\n",
    "gsutil mb -c standard -l ${REGION} gs://${BUCKET_NAME}\n",
    "```\n",
    "\n",
    "Then, build a container image:\n",
    "```bash\n",
    "export TEMPLATE_IMAGE=\"gcr.io/${PROJECT}/${TEMPLATE_NAME}:latest\"\n",
    "gcloud builds submit --tag \"${TEMPLATE_IMAGE}\" .\n",
    "```\n",
    "\n",
    "Create a metadata.json with the correct values and then create the Flex Template\n",
    "```bash\n",
    "export TEMPLATE_PATH=\"gs://${BUCKET_NAME}/${BUCKET_PATH}/${TEMPLATE_NAME}/${TEMPLATE_NAME}.json\"\n",
    "\n",
    "gcloud dataflow flex-template build ${TEMPLATE_PATH} \\\n",
    "  --image \"${TEMPLATE_IMAGE}\" \\\n",
    "  --sdk-language \"PYTHON\" \\\n",
    "  --metadata-file \"metadata.json\"\n",
    "```\n",
    "\n",
    "Finally execute the template in Dataflow and set the correct num of workers\n",
    "```bash\n",
    "gcloud dataflow flex-template run \"${TEMPLATE_NAME}\" \\\n",
    "    --template-file-gcs-location \"${TEMPLATE_PATH}\" \\\n",
    "    --region \"${REGION}\" \\\n",
    "    --parameters num_workers=10,max_num_workers=70,worker_region=${REGION}\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
