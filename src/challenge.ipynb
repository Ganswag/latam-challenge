{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este archivo puedes escribir lo que estimes conveniente. Te recomendamos detallar tu solución y todas las suposiciones que estás considerando. Aquí puedes ejecutar las funciones que definiste en los otros archivos de la carpeta src, medir el tiempo, memoria, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's start with a handmade aproach\n",
    "\n",
    "Let's suppose we are in a truly limited computer, but with infinite execution time\n",
    "\n",
    "But first, let's check a suposition about dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "FILE_PATH = './farmers-protest-tweets-2021-2-4.json'\n",
    "FILE_PATH_TEST = './farmers-protest-tweets-2021-2-4-test.json' # An smaller dataset :P\n",
    "READ_MODE = 'r'\n",
    "DUMMY_DATE = datetime.fromisoformat('2050-01-01T00:00:00+00:00').date()\n",
    "\n",
    "# Let's define a fictional date in the future to compare\n",
    "prev_date = DUMMY_DATE\n",
    "\n",
    "with open(FILE_PATH, 'r', encoding='utf-8', buffering=1) as json_file:\n",
    "    for line in json_file:\n",
    "        _line = json.loads(line)\n",
    "\n",
    "        date = datetime.fromisoformat(_line['date']).date()\n",
    "\n",
    "        if prev_date < date:  # Check if is sorted in a desc way\n",
    "            raise ValueError\n",
    "        \n",
    "        prev_date = date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this was executed without errors, we can affirm dates are always desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to create a method to insert in a sorted way elements, to avoid to order\n",
    "\n",
    "... and create some basic tests for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_sorted(iterable: list, element: tuple, el_idx: int) -> list:\n",
    "    i = 0\n",
    "    element[el_idx]  # check if element index exists and raise error if not\n",
    "    \n",
    "    if not iterable or element[el_idx] < iterable[-1][el_idx]:\n",
    "        iterable.append(element)\n",
    "    else:\n",
    "        while element[el_idx] < iterable[i][el_idx] and i < len(iterable):\n",
    "            i += 1\n",
    "        iterable.insert(i, element)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "from unittest import TestCase\n",
    "\n",
    "class TestFeaturesFlag(TestCase):\n",
    "\n",
    "    def test_insert_sorted_initial(self):\n",
    "        INITIAL_TEST_VALUE = []\n",
    "        NEW_DATA = ('a', 1)\n",
    "        EXPECTED_RESULT = [('a', 1)]\n",
    "\n",
    "        test_value = INITIAL_TEST_VALUE.copy()\n",
    "        insert_sorted(test_value, NEW_DATA, 0)\n",
    "        self.assertEqual(test_value, EXPECTED_RESULT)\n",
    "\n",
    "        test_value = INITIAL_TEST_VALUE.copy()\n",
    "        insert_sorted(test_value, NEW_DATA, 1)\n",
    "        self.assertEqual(test_value, EXPECTED_RESULT)\n",
    "\n",
    "        test_value = INITIAL_TEST_VALUE.copy()\n",
    "        self.assertRaises(IndexError, insert_sorted, test_value, NEW_DATA, 2)\n",
    "\n",
    "    def test_insert_sorted_begin(self):\n",
    "        INITIAL_TEST_VALUE = [('c', 3),  ('b', 2), ('a', 1)]\n",
    "        NEW_DATA = ('d', 4)\n",
    "        EXPECTED_RESULT = [('d', 4),('c', 3),  ('b', 2), ('a', 1)]\n",
    "\n",
    "        test_value = INITIAL_TEST_VALUE.copy()\n",
    "        insert_sorted(test_value, NEW_DATA, 0)\n",
    "        self.assertEqual(test_value, EXPECTED_RESULT)\n",
    "\n",
    "        test_value = INITIAL_TEST_VALUE.copy()\n",
    "        insert_sorted(test_value, NEW_DATA, 1)\n",
    "        self.assertEqual(test_value, EXPECTED_RESULT)\n",
    "\n",
    "        test_value = INITIAL_TEST_VALUE.copy()\n",
    "        self.assertRaises(IndexError, insert_sorted, test_value, NEW_DATA, 2)\n",
    "\n",
    "\n",
    "    def test_insert_sorted_normal(self):\n",
    "        INITIAL_TEST_VALUE = [('d', 4), ('b', 2), ('a', 1)]\n",
    "        NEW_DATA = ('c', 3)\n",
    "        EXPECTED_RESULT = [('d', 4), ('c', 3), ('b', 2), ('a', 1)]\n",
    "\n",
    "        test_value = INITIAL_TEST_VALUE.copy()\n",
    "        insert_sorted(test_value, NEW_DATA, 0)\n",
    "        self.assertEqual(test_value, EXPECTED_RESULT)\n",
    "\n",
    "        test_value = INITIAL_TEST_VALUE.copy()\n",
    "        insert_sorted(test_value, NEW_DATA, 1)\n",
    "        self.assertEqual(test_value, EXPECTED_RESULT)\n",
    "\n",
    "        test_value = INITIAL_TEST_VALUE.copy()\n",
    "        self.assertRaises(IndexError, insert_sorted, test_value, NEW_DATA, 2)\n",
    "\n",
    "\n",
    "    def test_insert_sorted_end(self):\n",
    "        INITIAL_TEST_VALUE = [('d', 4), ('c', 3), ('b', 2)]\n",
    "        NEW_DATA = ('a', 1)\n",
    "        EXPECTED_RESULT = [('d', 4),('c', 3),  ('b', 2), ('a', 1)]\n",
    "\n",
    "        test_value = INITIAL_TEST_VALUE.copy()\n",
    "        insert_sorted(test_value, NEW_DATA, 0)\n",
    "        self.assertEqual(test_value, EXPECTED_RESULT)\n",
    "\n",
    "        test_value = INITIAL_TEST_VALUE.copy()\n",
    "        insert_sorted(test_value, NEW_DATA, 1)\n",
    "        self.assertEqual(test_value, EXPECTED_RESULT)\n",
    "\n",
    "        test_value = INITIAL_TEST_VALUE.copy()\n",
    "        self.assertRaises(IndexError, insert_sorted, test_value, NEW_DATA, 2)\n",
    "\n",
    "\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, everything looks fine...\n",
    "And we'll need to get the max value from tuples inside the value in a dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_value(tweets_counter_by_user: dict) -> tuple:\n",
    "    \"\"\" Returns a tuple (key, value) for the maximum value in a dictionay\n",
    "\n",
    "    WARNING: Keep in mind this only will work for positive numbers because\n",
    "    tweet counter cannot be negative.\n",
    "    \"\"\"\n",
    "    max_value = ('__initial__', -1)\n",
    "    for user_name, tweets_counter in tweets_counter_by_user.items():\n",
    "        if tweets_counter > max_value[1]:  # This implies we'll get the first founded max\n",
    "            max_value = (user_name, tweets_counter)\n",
    "\n",
    "    return max_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "from unittest import TestCase\n",
    "\n",
    "class TestFeaturesFlag(TestCase):\n",
    "\n",
    "    def test_get_max(self):\n",
    "        TEST_OBJECT = {\n",
    "            'min': 1,\n",
    "            'dummy': 2,\n",
    "            'max': 3\n",
    "        }\n",
    "        EXPECTED_RESULT = ('max', 3)\n",
    "        result = get_max_value(TEST_OBJECT)\n",
    "\n",
    "        self.assertEqual(result, EXPECTED_RESULT)\n",
    "\n",
    "    def test_get_duplicated(self):\n",
    "        TEST_OBJECT = {\n",
    "            'min': 1,\n",
    "            'dummy': 2,\n",
    "            'max': 2\n",
    "        }\n",
    "        EXPECTED_RESULT = ('dummy', 2)\n",
    "        result = get_max_value(TEST_OBJECT)\n",
    "\n",
    "        self.assertEqual(result, EXPECTED_RESULT)\n",
    "\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_N = 10\n",
    "\n",
    "tweets_counter_by_user = {}\n",
    "max_user_by_date = []\n",
    "tweets_by_day = 0\n",
    "prev_date = DUMMY_DATE\n",
    "\n",
    "\n",
    "with open(FILE_PATH, 'r', encoding='utf-8', buffering=1) as json_file:\n",
    "    for line in json_file:\n",
    "        tweets_by_day += 1\n",
    "        _line = json.loads(line)\n",
    "\n",
    "        date = datetime.fromisoformat(_line['date']).date()\n",
    "        user = _line['user']['username']\n",
    "\n",
    "        if prev_date != date:  # this is because date is always descendent\n",
    "            max_value = get_max_value(tweets_counter_by_user)\n",
    "\n",
    "            insert_sorted(max_user_by_date, (prev_date, tweets_by_day - 1, max_value[0]), 1)\n",
    "            if len(max_user_by_date) > TOP_N:  # We only want TOP_N results\n",
    "                max_user_by_date.pop()\n",
    "\n",
    "            tweets_counter_by_user = {}\n",
    "            tweets_by_day = 1\n",
    "\n",
    "        prev_date = date\n",
    "\n",
    "        if tweets_counter_by_user.get(user):\n",
    "            tweets_counter_by_user[user] += 1\n",
    "        else:\n",
    "            tweets_counter_by_user[user] = 1\n",
    "    \n",
    "    # Compute data for last date:\n",
    "    max_value = get_max_value(tweets_counter_by_user)\n",
    "    insert_sorted(max_user_by_date, (prev_date, tweets_by_day - 1, max_value[0]), 1)\n",
    "    if len(max_user_by_date) > TOP_N:  # We only want TOP_N results\n",
    "        max_user_by_date.pop()\n",
    "\n",
    "\n",
    "result = [(el[0], el[2]) for el in max_user_by_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "mentions_counter = {}\n",
    "\n",
    "with open(FILE_PATH, 'r', encoding='utf-8', buffering=1) as json_file:\n",
    "    for line in json_file:\n",
    "        tweets_by_day += 1\n",
    "        _line = json.loads(line)\n",
    "        tweet_content = _line['renderedContent']\n",
    "        mentioned_users = re.findall(r'@(\\w+)\\b', tweet_content)\n",
    "\n",
    "        for user in mentioned_users:\n",
    "            if mentions_counter.get(user):\n",
    "                mentions_counter[user] += 1\n",
    "            else:\n",
    "                mentions_counter[user] = 1\n",
    "\n",
    "all_mentions_sorted = {\n",
    "    k: v for k, v in \n",
    "    sorted(mentions_counter.items(), key=lambda item: item[1], reverse=True)\n",
    "}\n",
    "top_n_mentions = [(k, all_mentions_sorted[k]) for k in list(all_mentions_sorted)[:TOP_N]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_mentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's copy the last solution but searching for emojis instead mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "\n",
    "\n",
    "emoji_counter = {}\n",
    "\n",
    "with open(FILE_PATH, 'r', encoding='utf-8', buffering=1) as json_file:\n",
    "    for line in json_file:\n",
    "        tweets_by_day += 1\n",
    "        _line = json.loads(line)\n",
    "        tweet_content = _line['renderedContent']\n",
    "\n",
    "        # This is trully slow, but since Indi characters are used I couldn't \n",
    "        # find a better solution far now :(\n",
    "        # I'd like to use a regex to find just emojies\n",
    "        used_emojis = [el['emoji'] for el in emoji.emoji_list(tweet_content)]\n",
    "\n",
    "        for _emoji in used_emojis:\n",
    "            if emoji_counter.get(_emoji):\n",
    "                emoji_counter[_emoji] += 1\n",
    "            else:\n",
    "                emoji_counter[_emoji] = 1\n",
    "\n",
    "sorted_emojies = {k: v for k, v in sorted(emoji_counter.items(), key=lambda item: item[1], reverse=True)}\n",
    "top_n_emojies = [(k, sorted_emojies[k]) for k in list(sorted_emojies)[:TOP_N]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_emojies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's suppose we have GCP and unlimited memory and procesors.\n",
    "\n",
    "Please note I don't believe to use GCP in this test could improve something since we are using an small dataset and limited computational resources, but this will be coded as if I wanted to use DataFlow and use as many workers as I wish, but instead read file from an Bucket, this time will be readed from a local path and the outoput just printed, but this could be stored in some GCP service easily, like BigQuery.\n",
    "\n",
    "Also remember the execution time will depends on the available computational resources, I believe the optimal solution for time and memory with limited resources are the previus ones, but lets use power that [apache beam](https://beam.apache.org/about/) offers.\n",
    "\n",
    "For more information about the execution model, take a look to [apache beam domentation](https://beam.apache.org/documentation/runtime/model/) about this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start with the easy one.\n",
    "\n",
    "Since multiple agregations are not required and we have a library to get emojis...\n",
    "Let's use a \"count words\" but with some extra steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apache_beam import CombinePerKey, Pipeline, FlatMap, Map\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "import apache_beam as beam\n",
    "\n",
    "import emoji\n",
    "\n",
    "def parse_element(element):\n",
    "    element = json.loads(element)\n",
    "    return element['renderedContent']\n",
    "\n",
    "\n",
    "def run(pipeline_args=None):\n",
    "    pipeline_options = PipelineOptions(\n",
    "        pipeline_args, save_main_session=True\n",
    "    )\n",
    "\n",
    "    with Pipeline(options=pipeline_options) as pipeline:\n",
    "        (\n",
    "            pipeline\n",
    "            | 'Read file' >> ReadFromText(FILE_PATH)  # This is faster than ReadFromJson\n",
    "            | 'Use the desired field' >> Map(parse_element)\n",
    "            # # Next line returns arrays:\n",
    "            | 'Get emojis' >> Map(lambda x: [el['emoji'] for el in emoji.emoji_list(x)])  # This is taking so much time :(\n",
    "            # # But next line flattens it:\n",
    "            | 'Flat elements' >> FlatMap(lambda x: x)\n",
    "            | 'Map to key / value' >> Map(lambda x: (x, 1))\n",
    "            | 'And sum by key' >> CombinePerKey(sum)\n",
    "            | \"Select top N\" >> beam.combiners.Top.Largest(10, key=lambda x: x[1])\n",
    "            | Map(print)\n",
    "        )\n",
    "\n",
    "pipeline_args = {'flexrs_goal': 'SPEED_OPTIMIZED'}  # Define this to avoid error\n",
    "run(pipeline_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "from apache_beam import CombinePerKey, Pipeline, FlatMap, Map, Regex\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "\n",
    "def run(pipeline_args=None):\n",
    "    pipeline_options = PipelineOptions(\n",
    "        pipeline_args, save_main_session=True,\n",
    "    )\n",
    "\n",
    "    with Pipeline(options=pipeline_options) as pipeline:\n",
    "        (\n",
    "            pipeline\n",
    "            beam.io.readF\n",
    "            | 'Read file' >> ReadFromText(FILE_PATH)\n",
    "            | 'Use the desired field' >> Map(parse_element)\n",
    "            # Next line returns arrays:\n",
    "            | 'Get mentions' >> Regex.find_all(r'(?<=@)(\\w+)\\b')\n",
    "            # But next line flattens it:\n",
    "            | 'Flat elements' >> FlatMap(lambda x: x)\n",
    "            | 'Map to (key, 1)' >> Map(lambda x: (x, 1))\n",
    "            | 'And sum by key' >> CombinePerKey(sum)\n",
    "            | \"Select top N\" >> beam.combiners.Top.Largest(TOP_N, key=lambda x: x[1])\n",
    "            | Map(print)\n",
    "            # ToDo: you could write it easily into BQ instead print it\n",
    "            # | WriteToBigQuery(\n",
    "            # project=project_id,\n",
    "            # table=table,\n",
    "            # schema=TABLE_SCHEMA,\n",
    "            # method=WriteToBigQuery.Method.STREAMING_INSERTS,  # To avoid use cloud storage \n",
    "            # insert_retry_strategy=RetryStrategy.RETRY_NEVER\n",
    "            # )\n",
    "        )\n",
    "\n",
    "pipeline_args = {'flexrs_goal': 'SPEED_OPTIMIZED'}  # Define this to avoid error\n",
    "run(pipeline_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But for get some top days with most activity and top user by day some extra ticks are needed, because we need to create a custom CombineFM, we take the solution from [Apache Beam documentation](https://beam.apache.org/documentation/transforms/python/aggregation/combinevalues/) looking in the [git repository](https://github.com/apache/beam/blob/master/sdks/python/apache_beam/examples/snippets/transforms/aggregation/combinevalues_combinefn.py) and adapt it to get what we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import apache_beam as beam\n",
    "from apache_beam import Pipeline, Map, GroupBy, CombineValues, CombineFn\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "\n",
    "def parse_element(element):\n",
    "    element = json.loads(element)\n",
    "    return (\n",
    "        str(datetime.fromisoformat(element['date']).date()),\n",
    "        element['user']['username']\n",
    "    )\n",
    "\n",
    "\n",
    "def format_output(element):\n",
    "   return [\n",
    "        (\n",
    "           datetime.strptime(el[0], '%Y-%m-%d').date(),\n",
    "           el[1][1]\n",
    "        ) for el in element\n",
    "    ]\n",
    "\n",
    "\n",
    "class MaxFn(beam.CombineFn):\n",
    "    def create_accumulator(self):\n",
    "      return {}\n",
    "\n",
    "    def add_input(self, accumulator, input):\n",
    "        if input not in accumulator:\n",
    "            accumulator[input] = 0\n",
    "        accumulator[input] += 1\n",
    "        return accumulator\n",
    "\n",
    "    def merge_accumulators(self, accumulators):\n",
    "      merged = {}\n",
    "      for accum in accumulators:\n",
    "        for item, count in accum.items():\n",
    "            if item not in merged:\n",
    "                merged[item] = 0\n",
    "            merged[item] += count\n",
    "      return merged\n",
    "\n",
    "    def extract_output(self, accumulator):\n",
    "        \"\"\"Customed solution to get the most active user by date\"\"\"\n",
    "        sum_tweets = 0\n",
    "\n",
    "        max_value = -1  # Initialize a dummy max value\n",
    "        most_active_user = None\n",
    "        for element, counter in accumulator.items():\n",
    "           sum_tweets += counter\n",
    "           if counter > max_value:\n",
    "                most_active_user = element[1]\n",
    "                max_value = counter\n",
    "\n",
    "        return sum_tweets, most_active_user\n",
    "\n",
    "\n",
    "def run(pipeline_args=None):\n",
    "    pipeline_options = PipelineOptions(pipeline_args)\n",
    "\n",
    "    with Pipeline(options=pipeline_options) as pipeline:\n",
    "        (\n",
    "            pipeline\n",
    "            # ToDo: You can replace read from local storage and read it from Cloud Storage using a gs://{path}\n",
    "            | 'Read file' >> ReadFromText(FILE_PATH)\n",
    "            | 'Use the desired field' >> Map(parse_element)\n",
    "            | 'Group all users by date' >> GroupBy(lambda x: x[0])\n",
    "            | 'Get most active user per date' >> CombineValues(MaxFn())\n",
    "            | 'Select Top 10' >> beam.combiners.Top.Largest(TOP_N, key=lambda x: x[1][0])\n",
    "            | 'Format to expected output' >> Map(format_output)\n",
    "            | 'Print results' >> Map(print)\n",
    "            # ToDo: you could write it easily into BQ instead print it\n",
    "            # | WriteToBigQuery(\n",
    "            # project=project_id,\n",
    "            # table=table,\n",
    "            # schema=TABLE_SCHEMA,\n",
    "            # method=WriteToBigQuery.Method.STREAMING_INSERTS,  # To avoid use cloud storage \n",
    "            # insert_retry_strategy=RetryStrategy.RETRY_NEVER\n",
    "            # )\n",
    "        )\n",
    "\n",
    "\n",
    "pipeline_args = {'flexrs_goal': 'SPEED_OPTIMIZED'}  # Define this to avoid error\n",
    "run(pipeline_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCP deployment instructions\n",
    "\n",
    "The apache beam solutions could be deployed using Flex Templates"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
